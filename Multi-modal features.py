# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YflNtNrJQI9LjPth6u9xeUz-la9pSF7y
"""

!pip install torch torchvision torchaudio
!pip install transformers pillow matplotlib
import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
from transformers import CLIPProcessor, CLIPModel
from google.colab import files
print("Libraries installed and imported!")

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
print("Model loaded successfully!")

uploaded = files.upload()

image_files = list(uploaded.keys())
image_files

texts_detailed = [
    "a small white Pomeranian walking at night on the street",
    "two fluffy white Bichon Frises sitting together in a stroller",
    "a phone screen showing assorted desserts on white plates",
    "a bouquet of pastel flowers including roses and hydrangeas",
    "a snowy empty street at night with street lamps",
    "a sunny street lined with power lines and green trees",
    "Japanese tsukemen noodles served with broth, pork, egg and seaweed",
    "a blue shopping bag and a wrapped gift box with a black leather cardholder beside them",
    "a hand holding a wrapped red rose in a subway station",
    "an airplane wing and terminal seen through a plane window at sunset",
]
texts = texts_detailed

texts_simple = [
    "a white dog at night",
    "two white dogs in a stroller",
    "desserts on plates",
    "a bouquet of flowers",
    "a snowy street",
    "a sunny street with trees",
    "a bowl of noodles",
    "a gift box and a cardholder",
    "a hand holding a rose",
    "a plane wing at an airport",
]
texts = texts_simple

texts_abstract = [
    "a quiet walk in the darkness",                         #img01_pomeranian
    "companionship and comfort while being moved",          #img02_bichon
    "a sweet indulgence and enjoyment",                     #img03_dessert
    "a gentle expression of affection and beauty",          #img04_bouquet
    "tranquility in a cold silent night",                   #img05_snowy
    "a warm peaceful outdoor afternoon",                    #img06_sunny
    "a savory moment of comfort and warmth",                #img07_tsukemen
    "a thoughtful gesture of gifting and care",             #img08_cardholder
    "a symbol of affection in a fleeting moment",           #img09_rose
    "a feeling of waiting before a long journey",           #img10_airplane
]
texts = texts_abstract

# Load and preprocess images
images = [Image.open(f).convert("RGB") for f in image_files]

inputs = processor(text=texts, images=images, return_tensors="pt", padding=True)

with torch.no_grad():
    outputs = model(**inputs)

# Separate embeddings
image_embeds = outputs.image_embeds
text_embeds = outputs.text_embeds

# Normalize embeddings (cosine similarity)
normalized_image_embeds = torch.nn.functional.normalize(image_embeds, p=2, dim=-1)
normalized_text_embeds = torch.nn.functional.normalize(text_embeds, p=2, dim=-1)

# Compute similarity matrix (10x10)
similarities = normalized_text_embeds @ normalized_image_embeds.T

# Print similarity matrix
print("Similarity matrix:\n")
for i in range(10):
  for j in range(10):
    print(similarities[i][j].item(), end=" ")
  print("\n")
print(similarities)

print("\n=== Ranking for each text query ===\n")

for i, text in enumerate(texts):
    sims = similarities[i]
    ranking = torch.argsort(sims, descending=True)
    print(f"\nText {i+1}: {text}")
    for r in ranking[:5]:
        print(f"   Image {r.item()+1} ({image_files[r.item()]})  --> similarity {sims[r]:.4f}")